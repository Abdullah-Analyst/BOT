# -*- coding: utf-8 -*-
"""Bot using RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1epBaJcnrIic180UBeTeZYDn26Ob_an1w
"""

!pip install requests beautifulsoup4 langchain transformers faiss-cpu

"""extra gpt code , copy of above code

"""

import requests
from bs4 import BeautifulSoup

urls = [
    "https://brandbonjour.com/about-us/",
    "https://brandbonjour.com/ai-tools/",
    "https://brandbonjour.com/blog/",
    "https://brandbonjour.com/brand-creatives/",
    "https://brandbonjour.com/success-stories/",
    "https://brandbonjour.com/client-onboarding-form/",
    "https://brandbonjour.com/consultation/",
    "https://brandbonjour.com/contact/",
    "https://brandbonjour.com/gmb/",
    "https://brandbonjour.com/hascon-homes/",
    "https://brandbonjour.com/",
    "https://brandbonjour.com/influencer-hub/",
    "https://brandbonjour.com/marketing-solutions/",
    "https://brandbonjour.com/privacy-policy/",
    "https://brandbonjour.com/projects/",
    "https://brandbonjour.com/script-form/",
    "https://brandbonjour.com/seo-digital-strategy/",
    "https://brandbonjour.com/service-feedback/",
    "https://brandbonjour.com/speaker-kit-request/",
    "https://brandbonjour.com/terms-conditions/",
    "https://brandbonjour.com/website-app-development/",
    "https://brandbonjour.com/who-we-are/",
    "https://brandbonjour.com/estelle-website-development-re-branding/"
]

all_texts = []

for link in urls:
    response = requests.get(link)
    if response.status_code == 200:
        print(f"Successfully retrieved content from {link}")
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        page_text = ' '.join([p.get_text() for p in paragraphs])
        all_texts.append(page_text)
    else:
        print(f"Failed to retrieve content from {link}")

# Preview first 1000 characters of all combined content
print("Preview of combined text:\n")
print(" ".join(all_texts)[:1000])

"""updated gpt code

"""

import requests
from bs4 import BeautifulSoup

urls = [
    "https://brandbonjour.com/about-us/",
    "https://brandbonjour.com/ai-tools/",
    "https://brandbonjour.com/blog/",
    "https://brandbonjour.com/brand-creatives/",
    "https://brandbonjour.com/success-stories/",
    "https://brandbonjour.com/client-onboarding-form/",
    "https://brandbonjour.com/consultation/",
    "https://brandbonjour.com/contact/",
    "https://brandbonjour.com/gmb/",
    "https://brandbonjour.com/hascon-homes/",
    "https://brandbonjour.com/",
    "https://brandbonjour.com/influencer-hub/",
    "https://brandbonjour.com/marketing-solutions/",
    "https://brandbonjour.com/privacy-policy/",
    "https://brandbonjour.com/projects/",
    "https://brandbonjour.com/script-form/",
    "https://brandbonjour.com/seo-digital-strategy/",
    "https://brandbonjour.com/service-feedback/",
    "https://brandbonjour.com/speaker-kit-request/",
    "https://brandbonjour.com/terms-conditions/",
    "https://brandbonjour.com/website-app-development/",
    "https://brandbonjour.com/who-we-are/",
    "https://brandbonjour.com/estelle-website-development-re-branding/"
]

all_texts = []

for url in urls:
    response = requests.get(url)

    if response.status_code == 200:
        print(f"✅ Retrieved: {url}")
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        text_content = ' '.join([para.get_text() for para in paragraphs])
        all_texts.append(text_content)
    else:
        print(f"❌ Failed: {url}")

print("\n✅ Scraping complete. Total pages scraped:", len(all_texts))

!pip install -U langchain-community langchain-openai

import os
from google.colab import userdata

# Set the OpenAI API key from Colab secrets
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

pip install --upgrade langchain openai

from langchain.embeddings.openai import OpenAIEmbeddings

"""updated gpt code

"""

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain.text_splitter import CharacterTextSplitter

# STEP 1: Convert scraped text to documents
documents = [Document(page_content=text) for text in all_texts]

# STEP 2: Split documents
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(documents)

# STEP 3: Embed + Store in FAISS
embeddings = OpenAIEmbeddings()
faiss_store = FAISS.from_documents(split_docs, embeddings)

# STEP 4: Set up LLM and chain
llm = OpenAI(temperature=0)
prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:
<context>
{context}
</context>
Question: {input}
Answer:""") # Added "Answer:" to the prompt


document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(faiss_store.as_retriever(), document_chain)

# STEP 5: Ask a question
query = "Tell me about their team members names"
response = retrieval_chain.invoke({"input": query})
print(response["answer"])

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/chat', methods=['POST'])
def chat():
    # Get user input
    user_input = request.json.get("input")

    # Run query using the retrieval chain
    response = retrieval_chain.invoke({"input": user_input})

    # Return the answer
    return jsonify({"answer": response["answer"]})

if __name__ == '__main__':
    app.run(debug=True)

!pip install gradio

import gradio as gr

def chat_with_bot(query):
    response = retrieval_chain.invoke({"input": query})
    return response["answer"]

# Create and launch the chatbot UI
gr.Interface(
    fn=chat_with_bot,
    inputs=gr.Textbox(lines=2, placeholder="Ask me anything about Brand Bonjour..."),
    outputs="text",
    title="Brand Bonjour AI Chatbot",
    description="Ask about services, projects, or anything else."
).launch()